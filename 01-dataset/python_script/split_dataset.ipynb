{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import numpy as np \n",
    "import kaggle\n",
    "from subprocess import call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we run the script where we have written dowload of the kaggle dataset by using kaggle api\n",
    "#the file will be downloaded to the current folder where you have this script \n",
    "rc = call(\"./script.sh\", shell=True)\n",
    "print(\"download successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are writing a fuction to help us load the file to aws s3 bucket \n",
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    print(\"uploaded {} to bucket {} \".format(object_name, bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here the function converts the file to parquet and uploads to the s3 bucket \n",
    "def convert_to_parquet(dataset):\n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i].to_parquet(f'data/ParisHousing_period_{i + 1:02d}.parquet')\n",
    "        \n",
    "        file_name = f\"/home/deen/Desktop/datatalks/Mlops-project/dataset/python_script/data/ParisHousing_period_{i + 1:02d}.parquet\"\n",
    "        bucket = \"mlops-project-dataset-deen\"\n",
    "        object_name = f\"paris-housing-dataset/ParisHousing_period_{i + 1:02d}.parquet\"\n",
    "        upload_file(file_name, bucket, object_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we state the location the of the downloaded file, sort it by date and reset the indexes \n",
    "path = \"./ParisHousingClass.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df.sort_values(by=['made'], inplace=True)\n",
    "df = df.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we split the file into four equal parts, so that we can act like the file comes at the end of every period\n",
    "np.split(df,4)\n",
    "period_01 = np.split(df,4)[0]\n",
    "period_02 = np.split(df,4)[1]\n",
    "period_03 = np.split(df,4)[2]\n",
    "period_04 = np.split(df,4)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we create inset the tables into an array called data \n",
    "data = [period_01,period_02,period_03,period_04]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "#we create a folder where we want the splitted data to enter\n",
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded paris-housing-dataset/ParisHousing_period_01.parquet to bucket mlops-project-dataset-deen \n",
      "uploaded paris-housing-dataset/ParisHousing_period_02.parquet to bucket mlops-project-dataset-deen \n",
      "uploaded paris-housing-dataset/ParisHousing_period_03.parquet to bucket mlops-project-dataset-deen \n",
      "uploaded paris-housing-dataset/ParisHousing_period_04.parquet to bucket mlops-project-dataset-deen \n"
     ]
    }
   ],
   "source": [
    "#we call the fuction \"convert to parquet that we created above \"\n",
    "convert_to_parquet(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
